{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":9858,"status":"ok","timestamp":1705623051050,"user":{"displayName":"Ákos Filakovszky","userId":"16949903218463042694"},"user_tz":-60},"id":"jMAyx8UnS73s"},"outputs":[],"source":["import os\n","import glob\n","import random\n","import pickle\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","\n","from tqdm import tqdm\n","from IPython import display\n","\n","from sklearn.model_selection import train_test_split\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.functional import relu\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset, TensorDataset\n","from torch.utils.tensorboard import SummaryWriter\n","\n","from torchvision import transforms\n","import torchvision.transforms as transforms\n","import torchvision.transforms.functional as TF"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1869,"status":"ok","timestamp":1705622969481,"user":{"displayName":"Ákos Filakovszky","userId":"16949903218463042694"},"user_tz":-60},"id":"kJ_jpX6foWMG","outputId":"d76c32e3-69f9-4c76-d23c-e4d6183afbf3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Connect to Google Drive on Google Colab\n","from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1705622969482,"user":{"displayName":"Ákos Filakovszky","userId":"16949903218463042694"},"user_tz":-60},"id":"QFO9Y3S_kX3V","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0251cbe6-3886-4cf3-b009-8dd9a6851c01"},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'cvproj' already exists and is not an empty directory.\n"]}],"source":["# Clone the GitHub repository\n","!git clone https://github.com/Elias-Buerger/cvproj.git"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1705623051050,"user":{"displayName":"Ákos Filakovszky","userId":"16949903218463042694"},"user_tz":-60},"id":"cjTjywjshM-P"},"outputs":[],"source":["DATA_PATH = \"./cvproj/data/\"\n","SAVE_PATH = \"drive/MyDrive/CV_V3/\""]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1705623051050,"user":{"displayName":"Ákos Filakovszky","userId":"16949903218463042694"},"user_tz":-60},"id":"UHUqwGBBvmXg"},"outputs":[],"source":["# NOTE: This code deletes the last batch (i.e., numpy file - including target, \"_y\" file)! The dataloader will NOT work otherwise!!!\n","files_to_delete = [\"332.npy\", \"332_y.npy\", \"670.npy\", \"670_y.npy\", \"1004.npy\", \"1004_y.npy\"]\n","for file_to_delete in files_to_delete:\n","    file_to_delete_path = os.path.join(DATA_PATH, file_to_delete)\n","    if os.path.exists(file_to_delete_path):\n","        os.remove(file_to_delete_path)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":668,"status":"ok","timestamp":1705623054528,"user":{"displayName":"Ákos Filakovszky","userId":"16949903218463042694"},"user_tz":-60},"id":"i1ipKFaUXXNc"},"outputs":[],"source":["class AOSDataset(Dataset):\n","    def __init__(self, file_list, target_path, random_aug=False):\n","        self.file_list = file_list\n","        self.target_path = target_path\n","        self.random_aug = random_aug\n","\n","    def __len__(self):\n","        return len(self.file_list) * 32\n","\n","    def preprocess_channel(self, channel):\n","        # Convert channel to PIL Image\n","        channel = Image.fromarray(channel)\n","\n","        # plt.imshow(channel, cmap=\"gray\")\n","        # plt.title(\"Original\")\n","        # plt.show()\n","\n","        # Contrast\n","        # channel_contrast = np.array(TF.adjust_contrast(channel, 100))\n","\n","        # plt.imshow(channel_contrast, cmap=\"gray\")\n","        # plt.title(\"Contrast\")\n","        # plt.show()\n","\n","        # Brightness\n","        # channel_brightness = np.array(self.color_jitter_brightness(channel))\n","\n","        # Inversion\n","        # channel_inverted = np.array(TF.invert(channel))\n","\n","        # Histogram Equalization\n","        channel_equalized = np.array(TF.equalize(channel))\n","\n","        # plt.imshow(channel_equalized, cmap=\"gray\")\n","        # plt.title(\"Equalization\")\n","        # plt.show()\n","\n","        # Inversion and Histogram Equalization\n","        # channel_inverted_equalized = np.array(TF.equalize(TF.invert(channel)))\n","\n","        return channel_equalized\n","\n","    def __getitem__(self, idx):\n","        file_idx = idx // 32 # Determine which file to load\n","        batch_idx = idx % 32 # Determine the index within the batch\n","\n","        data = np.load(self.file_list[file_idx])\n","        data = data[batch_idx]\n","\n","        target_idx = self.file_list[file_idx].split(\"/\")[-1].split(\".\")[0]\n","        target = np.load(os.path.join(self.target_path, f\"{target_idx}_y.npy\"))\n","        target = target[batch_idx]\n","\n","        # Apply preprocessing steps to each channel separately\n","        preprocessed_channels = []\n","        channel_ids = list(range(data.shape[0]))\n","        random.shuffle(channel_ids) # Randomize the order of channel_ids\n","        for channel_id in channel_ids:\n","            channel_data = data[channel_id]\n","\n","            prep_eq = self.preprocess_channel(channel_data)\n","\n","            preprocessed_channels.extend([channel_data, prep_eq])\n","\n","        data = np.array(preprocessed_channels)\n","\n","        del preprocessed_channels\n","\n","        # Normalize\n","        data = torch.Tensor(data) / 255.0\n","        target = torch.Tensor(target).unsqueeze(0) / 255.0 # 1 x ...\n","\n","        # Clip values to the range [0, 1]\n","        data = torch.clamp(data, 0, 1)\n","        target = torch.clamp(target, 0, 1)\n","\n","        # Apply random augmentation\n","        if self.random_aug:\n","            angle = random.choice([0, 90, 180, 270])\n","            data = TF.rotate(data, angle)\n","            target = TF.rotate(target, angle)\n","\n","            if random.random() > 0.5:\n","                data = TF.hflip(data)\n","                target = TF.hflip(target)\n","\n","            if random.random() > 0.5:\n","                data = TF.vflip(data)\n","                target = TF.vflip(target)\n","\n","        return data, target"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1705623055411,"user":{"displayName":"Ákos Filakovszky","userId":"16949903218463042694"},"user_tz":-60},"id":"O0dZ9WAPTN26"},"outputs":[],"source":["# Define the local paths for data (i.e., integrated images) and targets\n","data_path = DATA_PATH\n","target_path = DATA_PATH\n","\n","# Get all training files (ignoring \"_y.npy\")\n","file_list = glob.glob(os.path.join(data_path, \"*[!_y].npy\"))\n","\n","# Shuffle\n","random.seed(42)\n","random.shuffle(file_list)\n","\n","# Define train, test, validation splits\n","train_ratio = 0.8\n","test_ratio = 0.1\n","val_ratio = 0.1\n","\n","train_split = int(train_ratio * len(file_list))\n","test_split = train_split + int(test_ratio * len(file_list))\n","\n","train_file_list = file_list[:train_split]\n","test_file_list = file_list[train_split:test_split]\n","val_file_list = file_list[test_split:]\n","\n","# Create AOSDataset instances\n","train_dataset = AOSDataset(train_file_list, target_path, random_aug=True)\n","test_dataset = AOSDataset(test_file_list, target_path)\n","val_dataset = AOSDataset(val_file_list, target_path)\n","\n","# Create DataLoader instances\n","train_loader = DataLoader(train_dataset, batch_size=6, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=6)\n","val_loader = DataLoader(val_dataset, batch_size=6)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1705623056681,"user":{"displayName":"Ákos Filakovszky","userId":"16949903218463042694"},"user_tz":-60},"id":"uZjBmNTeUQc7"},"outputs":[],"source":["# Retrieve samples if needed...\n","# for sample in train_loader:\n","#     sample"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1705623057457,"user":{"displayName":"Ákos Filakovszky","userId":"16949903218463042694"},"user_tz":-60},"id":"dWfqcxD3jHk1"},"outputs":[],"source":["class DoubleConv(nn.Module):\n","    def __init__(self, in_channels, out_channels, mid_channels=None, kernel_size=3, padding=1):\n","        super().__init__()\n","        if not mid_channels:\n","            mid_channels = out_channels\n","        self.double_conv = nn.Sequential(\n","            nn.Conv2d(in_channels, mid_channels, kernel_size=kernel_size, padding=padding),\n","            nn.BatchNorm2d(mid_channels),\n","            nn.SiLU(inplace=True),\n","            nn.Conv2d(mid_channels, out_channels, kernel_size=kernel_size, padding=padding),\n","            nn.BatchNorm2d(out_channels),\n","            nn.SiLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","        return self.double_conv(x)\n","\n","\n","class Down(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.maxpool_conv = nn.Sequential(\n","            nn.MaxPool2d(2),\n","            DoubleConv(in_channels, out_channels)\n","        )\n","\n","    def forward(self, x):\n","        return self.maxpool_conv(x)\n","\n","\n","class Up(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","\n","        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n","        self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n","\n","        self.gate = nn.Conv2d(in_channels // 2, in_channels // 2, kernel_size=3, padding=1)\n","\n","    def forward(self, x1, x2):\n","        x1 = self.up(x1)\n","\n","        diffY = x2.size()[2] - x1.size()[2]\n","        diffX = x2.size()[3] - x1.size()[3]\n","\n","        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n","                        diffY // 2, diffY - diffY // 2])\n","\n","        x2 = x2 * torch.sigmoid(self.gate(x1))\n","\n","        x = torch.cat([x2, x1], dim=1)\n","\n","        return self.conv(x)\n","\n","\n","class Out(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(Out, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n","    def forward(self, x):\n","        return self.conv(x)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1705623059311,"user":{"displayName":"Ákos Filakovszky","userId":"16949903218463042694"},"user_tz":-60},"id":"tqfaiQErixjn"},"outputs":[],"source":["# Gated U-Net with Bilinear Upsampling\n","# U-Net Architecture Adapted from https://github.com/milesial/Pytorch-UNet\n","class GatedUNetWithBilinearUpsampling(nn.Module):\n","    def __init__(self, n_channels_in, n_channels_out, bilinear=True):\n","        super(GatedUNetWithBilinearUpsampling, self).__init__()\n","        self.n_channels_in = n_channels_in\n","        self.n_channels_out = n_channels_out\n","        self.bilinear = bilinear\n","\n","        self.inc = DoubleConv(n_channels_in, 64)\n","\n","        self.down1 = Down(64, 128)\n","        self.down2 = Down(128, 256)\n","        self.down3 = Down(256, 512)\n","        self.down4 = Down(512, 1024 // 2) # // 2 -> Bilinear Upsampling\n","\n","        self.up1 = Up(1024, 512 // 2)\n","        self.up2 = Up(512, 256 // 2)\n","        self.up3 = Up(256, 128 // 2)\n","        self.up4 = Up(128, 64)\n","\n","        self.out = Out(64, n_channels_out)\n","\n","    def forward(self, x):\n","        x1 = self.inc(x)\n","\n","        x2 = self.down1(x1)\n","        x3 = self.down2(x2)\n","        x4 = self.down3(x3)\n","        x5 = self.down4(x4)\n","\n","        x = self.up1(x5, x4)\n","        x = self.up2(x, x3)\n","        x = self.up3(x, x2)\n","        x = self.up4(x, x1)\n","\n","        x = self.out(x)\n","\n","        return x"]},{"cell_type":"code","source":["def save_batch_images(data, save_path, prefix):\n","    os.makedirs(save_path, exist_ok=True)\n","\n","    for item_id in range(len(data)):\n","        item_data = data[item_id].cpu().numpy()\n","\n","        for channel_id in range(item_data.shape[0]):\n","            channel_data = item_data[channel_id]\n","\n","            # Clip values between 0 and 1 and scale to [0, 255]\n","            channel_data = np.clip(channel_data, 0, 1)\n","            channel_data = (channel_data * 255).astype(np.uint8)\n","\n","            # Convert numpy array to PIL Image\n","            image = Image.fromarray(channel_data, mode=\"L\")\n","\n","            # Create a folder for each item in the batch\n","            folder_path = os.path.join(save_path, f\"{prefix}_{item_id + 1}\")\n","            os.makedirs(folder_path, exist_ok=True)\n","\n","            # Save the image as PNG\n","            image.save(os.path.join(folder_path, f\"channel_{channel_id + 1}.png\"))\n","\n","            del channel_data, image\n","\n","        del item_data"],"metadata":{"id":"fFHE2qEl8FcQ","executionInfo":{"status":"ok","timestamp":1705623060819,"user_tz":-60,"elapsed":5,"user":{"displayName":"Ákos Filakovszky","userId":"16949903218463042694"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iFHhEcw-TQVt","outputId":"11659cad-0806-44ab-8cfb-8b14774ea1c7"},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["[TRAIN] Epoch 1/50: 100%|██████████| 4272/4272 [2:10:34<00:00,  1.83s/Batches]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Model saved at epoch: 0\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["[TEST] Epoch 1/50: 100%|██████████| 534/534 [03:05<00:00,  2.89Batches/s]\n","[VALIDATION] Epoch 1/50: 100%|██████████| 539/539 [03:07<00:00,  2.88Batches/s]\n","[TRAIN] Epoch 2/50: 100%|██████████| 4272/4272 [2:14:41<00:00,  1.89s/Batches]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Model saved at epoch: 1\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["[TEST] Epoch 2/50: 100%|██████████| 534/534 [03:02<00:00,  2.93Batches/s]\n","[VALIDATION] Epoch 2/50: 100%|██████████| 539/539 [03:03<00:00,  2.93Batches/s]\n","[TRAIN] Epoch 3/50: 100%|██████████| 4272/4272 [2:15:47<00:00,  1.91s/Batches]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Model saved at epoch: 2\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["[TEST] Epoch 3/50: 100%|██████████| 534/534 [03:05<00:00,  2.87Batches/s]\n","[VALIDATION] Epoch 3/50: 100%|██████████| 539/539 [03:07<00:00,  2.87Batches/s]\n","[TRAIN] Epoch 4/50: 100%|██████████| 4272/4272 [2:18:21<00:00,  1.94s/Batches]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Model saved at epoch: 3\n"]},{"output_type":"stream","name":"stderr","text":["[TEST] Epoch 4/50: 100%|██████████| 534/534 [03:04<00:00,  2.89Batches/s]\n","[VALIDATION] Epoch 4/50: 100%|██████████| 539/539 [03:05<00:00,  2.90Batches/s]\n","[TRAIN] Epoch 5/50: 100%|██████████| 4272/4272 [2:19:30<00:00,  1.96s/Batches]\n"]},{"output_type":"stream","name":"stdout","text":["Model saved at epoch: 4\n"]},{"output_type":"stream","name":"stderr","text":["[TEST] Epoch 5/50: 100%|██████████| 534/534 [03:06<00:00,  2.87Batches/s]\n","[VALIDATION] Epoch 5/50: 100%|██████████| 539/539 [03:09<00:00,  2.85Batches/s]\n","[TRAIN] Epoch 6/50: 100%|██████████| 4272/4272 [2:19:40<00:00,  1.96s/Batches]\n"]},{"output_type":"stream","name":"stdout","text":["Model saved at epoch: 5\n"]},{"output_type":"stream","name":"stderr","text":["[TEST] Epoch 6/50: 100%|██████████| 534/534 [03:06<00:00,  2.86Batches/s]\n","[VALIDATION] Epoch 6/50: 100%|██████████| 539/539 [03:09<00:00,  2.85Batches/s]\n","[TRAIN] Epoch 7/50: 100%|██████████| 4272/4272 [2:17:01<00:00,  1.92s/Batches]\n"]},{"output_type":"stream","name":"stdout","text":["Model saved at epoch: 6\n"]},{"output_type":"stream","name":"stderr","text":["[TEST] Epoch 7/50: 100%|██████████| 534/534 [03:05<00:00,  2.88Batches/s]\n","[VALIDATION] Epoch 7/50: 100%|██████████| 539/539 [03:09<00:00,  2.85Batches/s]\n","[TRAIN] Epoch 8/50:   6%|▌         | 235/4272 [07:57<2:12:38,  1.97s/Batches]"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model = GatedUNetWithBilinearUpsampling(6, 1).to(device)\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n","\n","# Track losses\n","train_losses = []\n","test_losses = []\n","val_losses = []\n","\n","# Training loop\n","num_epochs = 50\n","for epoch in range(num_epochs):\n","    train_loss = 0.0\n","    model.train()\n","    for batch_idx, (batch_data, batch_targets) in enumerate(tqdm(train_loader, desc=f\"[TRAIN] Epoch {epoch+1}/{num_epochs}\", unit=\"Batches\")):\n","        batch_data, batch_targets = batch_data.to(device), batch_targets.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(batch_data)\n","        loss = criterion(outputs, batch_targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","\n","        del loss, batch_data, batch_targets, outputs\n","        # torch.cuda.empty_cache()\n","\n","    # Calculate average training loss for the epoch\n","    avg_train_loss = train_loss / len(train_loader)\n","    train_losses.append(avg_train_loss)\n","\n","    # Save model weights\n","    model_save_path = f\"{SAVE_PATH}unet_epoch_{epoch}.pth\"\n","    torch.save(model.state_dict(), model_save_path)\n","    print(f\"Model saved at epoch: {epoch}\")\n","\n","    # Update learning rate\n","    scheduler.step()\n","\n","    # Test\n","    test_loss = 0.0\n","    model.eval()\n","    with torch.no_grad():\n","        for batch_idx, (batch_data, batch_targets) in enumerate(tqdm(test_loader, desc=f\"[TEST] Epoch {epoch+1}/{num_epochs}\", unit=\"Batches\")):\n","            batch_data, batch_targets = batch_data.to(device), batch_targets.to(device)\n","            outputs = model(batch_data)\n","            loss = criterion(outputs, batch_targets)\n","\n","            test_loss += loss.item()\n","\n","            # Save some batches as examples for the report...\n","            if (batch_idx + 1) % (len(test_loader) // 3) == 0:\n","                # Save batch_data, batch_targets, and outputs as images\n","                save_batch_images(batch_data, f\"{SAVE_PATH}test_batch_data_{epoch + 1}\", \"batch_data\")\n","                save_batch_images(batch_targets, f\"{SAVE_PATH}test_batch_targets_{epoch + 1}\", \"batch_targets\")\n","                save_batch_images(outputs, f\"{SAVE_PATH}test_batch_outputs_{epoch + 1}\", \"outputs\")\n","\n","\n","            del loss, batch_data, batch_targets, outputs\n","            # torch.cuda.empty_cache()\n","\n","    # Calculate average test loss for the epoch\n","    avg_test_loss = test_loss / len(test_loader)\n","    test_losses.append(avg_test_loss)\n","\n","    # Validation\n","    val_loss = 0.0\n","    model.eval()\n","    with torch.no_grad():\n","        save_counter = 0\n","        for batch_idx, (batch_data, batch_targets) in enumerate(tqdm(val_loader, desc=f\"[VALIDATION] Epoch {epoch+1}/{num_epochs}\", unit=\"Batches\")):\n","            batch_data, batch_targets = batch_data.to(device), batch_targets.to(device)\n","            outputs = model(batch_data)\n","            loss = criterion(outputs, batch_targets)\n","\n","            val_loss += loss.item()\n","\n","            # Save some batches as examples for the report...\n","            if (batch_idx + 1) % (len(val_loader) // 3) == 0:\n","                # Save batch_data, batch_targets, and outputs as images\n","                save_batch_images(batch_data, f\"{SAVE_PATH}val_batch_data_{epoch + 1}\", \"batch_data\")\n","                save_batch_images(batch_targets, f\"{SAVE_PATH}val_batch_targets_{epoch + 1}\", \"batch_targets\")\n","                save_batch_images(outputs, f\"{SAVE_PATH}val_batch_outputs_{epoch + 1}\", \"outputs\")\n","\n","            del loss, batch_data, batch_targets, outputs\n","            # torch.cuda.empty_cache()\n","\n","    # Calculate average validation loss for the epoch\n","    avg_val_loss = val_loss / len(val_loader)\n","    val_losses.append(avg_val_loss)\n","\n","    # Plot and save losses\n","    plt.figure(figsize=(10, 5))\n","    plt.plot(range(1, epoch + 2), train_losses, label=\"Train Loss\", marker=\"o\")\n","    plt.plot(range(1, epoch + 2), val_losses, label=\"Validation Loss\", marker=\"o\")\n","    plt.title(\"Training and Validation Loss\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig(f\"{SAVE_PATH}loss_plot_{epoch + 1}.png\")\n","    plt.close()\n","\n","    loss_dict = {\"train_losses\": train_losses, \"test_losses\": test_losses, \"val_losses\": val_losses}\n","    with open(f\"{SAVE_PATH}losses_{epoch + 1}.pkl\", \"wb\") as file:\n","        pickle.dump(loss_dict, file)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}